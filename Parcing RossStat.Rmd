---
title: "Parcing RossStat"
author: "Gergel Anastasia"
output: 
  html_document:
    latex_engine: xelatex
header-includes: 
- \usepackage[T2A]{fontenc}
- \usepackage[utf8]{inputenc}
- \usepackage[russian]{babel}
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# set the working directory
setwd("/Users/herrhellana/Documents/Research Labs/МЦИИР/Parcing")
```


Database: "Russian Regional Yearbook"
`https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_index0.htm`. 

Homepage: 
`https://uisrussia.msu.ru/stat/Publications/Publications.htm`.

(1) Function for scraping RossStat tables with a specified url. 
```{r}
library(rvest)
library(stringr)

# ------------------------- SCRAPE THE TABLE -------------------------- 
scrapeRossStatTable <- function(url){
  ##############################################
  # returns a dataset of the parsed html table #
  # based on library(rvest) #
  
  html <- read_html(url) # parse the html code
  
  # (1) read the html table
  tab <- html_table(html, fill=TRUE)
  tab <- tab[[1]]

  # (2) cleaning
  # to simplify the further analysis we can replace commas with dots in scientific notation
  tab <- sapply(tab, gsub, pattern = ",", replacement= ".")
  colnames(tab) <- tab[1,] # change names of columns
  tab <- tab[-1, ] # delete the duplicates
  
  return(tab)
}

# test the function for several links
url <- "https://uisrussia.msu.ru/stat/Publications/Ejeg2016/Ejeg2016_04_/Ejeg2016_04_240.htm"
url2 <- "https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_05_/Ejeg2017_05_010.htm"
url3 <- "https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_05_/Ejeg2017_05_030.htm"
url4 <- "https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_05_/Ejeg2017_05_260.htm"
url5 <- "https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_05_/Ejeg2017_05_290.htm"
data <- scrapeRossStatTable(url5)
head(data)
```



(2) Save the table to a csv file with a specific name. Be aware that functions `scrapeRossStatTable`, `GetTableName`, and `saveCSV` should be coerced to work effeciently. I will perform it a bit later. 
```{r}
# -------------------- Get the name of the table -------------------
GetTableName <- function(url){
  html <- read_html(url) # parse the html code
  # (1) extracting the name of the table
  TableNameRaw <- as.character(html %>% 
                                 html_nodes(xpath = "/html/body/comment()[3]"))
  first_split <- strsplit(TableNameRaw, "<title>")[[1]][2]
  TableName <- strsplit(first_split, "</title>")[[1]][1] # cleaned name
}



# INSERT YOURS PATH IF CURRENT WORKING DIRECTORY IS NOT THE PATH NEEDED
WD <- getwd()

# --------------------- SAVE THE TABLE TO CSV --------------------- 
saveCSV <- function(data, WD){
  PathName <- paste0(WD, "/", TableName, ".csv") # WD + file name
  write.csv(file = PathName, x = data) # save to a csv file
}
```



(3) Now we can coerce all these functions to a new one that parses the html code, scrapes the table, and saves it in a csv file. 
```{r}
save_scrapeRossStatTable <- function(url, WD = getwd()){
  ##############################################
  # returns a dataset of the parsed html table #
  # additionally, saves the table to a csv file #
  # WD is a current working directory by default #
  # based on library(rvest) #
  
  html <- read_html(url) # parse the html code
  
  # (1) read the html table
  tab <- html_table(html, fill=TRUE)
  tab <- tab[[1]]

  # (2) cleaning
  # to simplify the further analysis we can replace commas with dots in scientific notation
  tab <- sapply(tab, gsub, pattern = ",", replacement= ".")
  colnames(tab) <- tab[1,] # change names of columns
  tab <- tab[-1, ] # delete the duplicates
  
  # (3) extracting the name of the table
  TableNameRaw <- as.character(html %>% 
                                 html_nodes(xpath = "/html/body/comment()[3]"))
  first_split <- strsplit(TableNameRaw, "<title>")[[1]][2]
  TableName <- strsplit(first_split, "</title>")[[1]][1] # cleaned name
  
  PathName <- paste0(WD, "/", TableName, ".csv") # WD + file name
  write.csv(file = PathName, x = tab) # save to a csv file
  
  # return(tab)
}
```

I've commented `return(tab)` so the function only saves the tables. If you want to save the table into your WD, you can uncomment this line. 



(4. a) Function that saves links to all tables and its names in a specific section. 

One can scrape all the tables of a section by refering to this dataset, or choose specific tables by its names. Then, save these tables of interest to a vector and apply the `save_scrapeRossStatTable` function to each element of the vector. 

```{r}
# Section "Labor"
url <- "https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_05_/Ejeg2017_05_000.htm#HL_2"
region_url <- "https://uisrussia.msu.ru/stat/Publications/Reg2017-2/Reg2017-2_03_/Reg2017-2_03_000.htm"


# ----------------- Section Info FUNCTION --------------- #
getSectionInfo <- function(url){
  # takes a url of one section #
  # returns an efficient dataset with links to tables and its names #
  
  section <- read_html(url) # parse the html code 
  tabnames <- section %>% html_nodes(css = ".HLNormal li") # parse the part containing links to tables alone
  
  # save all links into a vector
  links <- str_match_all(tabnames, "<a href=\"(.*?)\"") # find them all
  
  section_urls <- rep(0, length(links)) # create a null vector where to save final links
  section_tables <- rep(0, length(links)) # create a null vector where to save table names
  
  ################
  # FOR REGIONAL BOOK
  for (i in 1:length(links)){
  section_urls[i] <- paste0("https://uisrussia.msu.ru/stat/Publications/Reg2017-2/Reg2017-2_03_/", # append web-domain to other links
                    links[[i]][,2]) # loop over raw data
  
  section_tables[i] <- strsplit(html_text(tabnames), "\n")[[i]][2]
  # delete "\n" from table names and save a resulting string
  }
  
  
  ################
  # FOR YEARBOOK
  # for (i in 1:length(links)){
  #section_urls[i] <- paste0("https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_05_/", # append web-domain to other links
 #                   links[[i]][,2]) # loop over raw data
  
  #section_tables[i] <- strsplit(html_text(tabnames), "\n")[[i]][2]
  # delete "\n" from table names and save a resulting string
  #}
  
  # https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_05_/
  # where `Ejeg2017_05_` is a section ID
  # `Ejeg2017` is for a 2017 Yearbook
  
  # effecient dataset
  return(cbind(section_tables, section_urls))
}
```






(5) Similar codings for the home page. Now, scrape urls of sections. 

```{r}
home_url <- "https://uisrussia.msu.ru/stat/Publications/Ejeg2017/Ejeg2017_index0.htm" # 2017 Yearbook
reg_url <- "https://uisrussia.msu.ru/stat/Publications/Reg2017-2/Reg2017-2_index0.htm" # Regional Book 2017


# ------------------- Home Info FUNCTION -------------------- #
getHomePageInfo <- function(YEAR){
  # takes year of Yearbook to scrape #
  # returns an efficient dataset with links to sections and its names #
  ################################
  
  #home_url <- paste0("https://uisrussia.msu.ru/stat/Publications/Ejeg", as.character(YEAR), "/Ejeg", as.character(YEAR), "_index0.htm")
  # for Yearbook
  
  home_url <- paste0("https://uisrussia.msu.ru/stat/Publications/Reg", as.character(YEAR), "-2/Reg", as.character(YEAR), "-2_index0.htm")
  # for Regional Book
  
  home_page <- read_html(home_url) # parse html code
  home_parsed <- home_page %>% html_nodes(css = "p") # urls and names
  

  # ------- URLS of sections ------ #
  home_links <- str_match_all(home_parsed, "<a href=\"(.*?)\"")
  home_links <- home_links[3:length(home_links)] # the first link is null, the second one leads to a preface, delete it
  home_sections_urls <- rep(0, length(home_links)) # create a null vector where to save final links
  
  # ------- NAMES of sections ------ #
  home_sections <- home_page %>% html_nodes(css = ".HLMenu4")
  home_sections <- home_sections[2:length(home_sections)] # the first is a preface, delete it
  home_sections_names <- rep(0, length(home_links)) # create a null vector where to save table names
  
  
  # FOR REGIONAL BOOK
  ####################
  for (i in 1:length(home_links)){
    home_sections_urls[i] <-  paste0("https://uisrussia.msu.ru/stat/Publications/Reg2017-2/", 
                                    # append web-domain to other links
                                   home_links[[i]][,2]) # loop over raw data
    home_sections_names[i] <- strsplit(html_text(home_sections), "\n")[[i]][2]
    # delete "\n" from table names and save a resulting string
  }  
  
  
  
  # FOR YEARBOOK
  ################
  #for (i in 1:length(home_links)){
  #  home_sections_urls[i] <-  paste0("https://uisrussia.msu.ru/stat/Publications/Ejeg2017/", 
                                    # append web-domain to other links
 #                                  home_links[[i]][,2]) # loop over raw data
  #  home_sections_names[i] <- strsplit(html_text(home_sections), "\n")[[i]][2]
    # delete "\n" from table names and save a resulting string
  #}
  
  return(cbind(home_sections_names, home_sections_urls))
}
```



Check the results. 

Imagine we would download tables of the Labor section from 2017 Yearbook. 

```{r}
data <- getHomePageInfo(2017) # first find out which sections do we have
data[, 1] # print sections, Labor is the 3d element
YOURurl <- data[3, 2] # get Labor's url and save it

LaborInfo <- getSectionInfo(YOURurl)  # we've called a dataset that contains table names of the section, and its urls

# now we can save e.g. the first 10 tables to our WD
sapply(LaborInfo[1:10, 2], save_scrapeRossStatTable)


######################################################
# here I scrape the first table in a section "Labor" and change the WD where I save the table
save_scrapeRossStatTable(LaborInfo[1, 2], WD = "/Users/herrhellana/Documents/Research Labs")
```

![](screen.png)
